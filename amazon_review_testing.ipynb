{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175e7786-8e51-4e84-849b-9d00e5d4c92a",
   "metadata": {},
   "source": [
    "# Recommender System with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "951e16ec",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "983b6c02",
   "metadata": {},
   "source": [
    "### Packages & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc723e1-d85f-40b3-91fa-0e8f85395711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import typing\n",
    "\n",
    "import urllib.request\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import gzip\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9021e6d4",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83509cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.join(\"D:\" + os.sep, \"Code\", \"PYTHON\", \"Amazon_Recommender_System\")\n",
    "\n",
    "CODE_DIR = os.path.join(MAIN_DIR, \"Code\")\n",
    "\n",
    "ANALYSIS_DIR = os.path.join(MAIN_DIR, \"Analysis\")\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"Data\")\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"Raw\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_DIR, \"Clean\")\n",
    "\n",
    "BOOKS_RAW_DIR = os.path.join(RAW_DATA_DIR, \"Books\")\n",
    "BOOKS_CLEAN_DIR = os.path.join(CLEAN_DATA_DIR, \"Books\")\n",
    "BOOKS_SENTIMENT_DIR = os.path.join(BOOKS_CLEAN_DIR, \"Sentiment\")\n",
    "\n",
    "CHUNK_SIZE = 1000000\n",
    "\n",
    "NUM_CORES = math.ceil(mp.cpu_count()/2)\n",
    "\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71a50b4e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a6c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_list: list) -> None:\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def is_directory_empty(dir_path: str) -> bool:\n",
    "    return len(os.listdir(dir_path)) == 0\n",
    "\n",
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, tf.random.set_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3231a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory([CODE_DIR, \n",
    "                  ANALYSIS_DIR,\n",
    "                  DATA_DIR, \n",
    "                  RAW_DATA_DIR, \n",
    "                  CLEAN_DATA_DIR,\n",
    "                  BOOKS_RAW_DIR,\n",
    "                  BOOKS_CLEAN_DIR,\n",
    "                  BOOKS_SENTIMENT_DIR,\n",
    "                  ])\n",
    "\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53f8033",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca015dd",
   "metadata": {},
   "source": [
    "### Loading Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc72a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More memory efficient version. Takes longer but is far more consistent than the previous version. \n",
    "def process_data(url: str, \n",
    "                 chunk_size: int, \n",
    "                 num_workers: int,\n",
    "                 output_dir: str) -> None:\n",
    "    \n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with gzip.open(response, \"rt\") as gz_file:\n",
    "            chunk = []\n",
    "            chunk_count = 0\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                for line in gz_file:\n",
    "                    chunk.append(line)\n",
    "\n",
    "                    if len(chunk) == chunk_size:\n",
    "                        executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "                        chunk = []\n",
    "                        chunk_count += 1\n",
    "\n",
    "                # Process the remaining lines in the last chunk\n",
    "                if chunk:\n",
    "                    executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "\n",
    "def process_chunk(chunk: list,\n",
    "                  filename: str) -> None:\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        file.writelines(chunk)\n",
    "    print(f\"Processed chunk: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099c2af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m----- PROCESS_DATA -----\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mGAME URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m1. 1000000 chunk -> 22min 22sec\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     19\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m process_data(url, CHUNK_SIZE, NUM_CORES, BOOKS_RAW_DIR)\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(url, chunk_size, num_workers, output_dir)\u001b[0m\n\u001b[0;32m     10\u001b[0m chunk_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mnum_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m gz_file:\n\u001b[0;32m     14\u001b[0m         chunk\u001b[39m.\u001b[39mappend(line)\n\u001b[0;32m     16\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chunk) \u001b[39m==\u001b[39m chunk_size:\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\gzip.py:314\u001b[0m, in \u001b[0;36mGzipFile.read1\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    313\u001b[0m     size \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 314\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mread1(size)\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\gzip.py:505\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_member \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m--> 505\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(io\u001b[39m.\u001b[39;49mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m    507\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39mdecompress(buf, size)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\gzip.py:97\u001b[0m, in \u001b[0;36m_PaddedFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     94\u001b[0m read \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[read:] \u001b[39m+\u001b[39m \\\n\u001b[1;32m---> 97\u001b[0m        \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile\u001b[39m.\u001b[39;49mread(size\u001b[39m-\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_length\u001b[39m+\u001b[39;49mread)\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "----- PROCESS_DATA -----\n",
    "GAME URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "BOOK URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "EXPERIMENTS FOR 27 MILLION\n",
    "----- THREAD CSV_FILE -----\n",
    "1. 100000 chunk -> 22min 26sec\n",
    "\n",
    "----- THREAD JSON_FILE RUNTIMES-----\n",
    "1. 100000 chunk -> 20min 55sec\n",
    "2. 100000 chunk -> 49min 1sec -> No idea why this happened on a fresh start\n",
    "3. 500000 chunk -> 18min 45sec\n",
    "4. 1000000 chunk -> 17min 5sec\n",
    "\n",
    "NEW STABLE IMPLEMENTATION\n",
    "1. 1000000 chunk -> 22min 22sec\n",
    "'''\n",
    "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "process_data(url, CHUNK_SIZE, NUM_CORES, BOOKS_RAW_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe328f04",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f706d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(dictionary: dict):\n",
    "    return dictionary.get(\"Format:\") if isinstance(dictionary, dict) else None\n",
    "\n",
    "def filter_comment_length(reviews: pd.DataFrame, \n",
    "                          minimum: int) -> pd.DataFrame:\n",
    "    reviews = reviews.copy()\n",
    "    reviews[\"review_len\"] = reviews[\"review_text\"].str.split().str.len()\n",
    "    reviews = reviews.loc[(reviews[\"review_len\"] > minimum)]\n",
    "    return reviews\n",
    "\n",
    "def remove_symbols(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"\\s+|[^a-zA-Z0-9\\s]\", \"\", x)) # Cleans up duplicate space and special characters.\n",
    "\n",
    "def remove_irrelevant_info(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"http\\S+|www.\\S+|#\\S+|<.*?>|\\(|\\)|\\d+\", \"\", x)) # Cleans up URL, hashtags, parenthesis, and numbers.\n",
    "\n",
    "def reduce_characters(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"[^\\w\\s]|(.)\\1+\", \"\", x)) # Remove excessive punctuation and repeated characters\n",
    "\n",
    "def normalize_encoding(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")) # Encodes and Decodes the data so that we have consistency in text\n",
    "        \n",
    "def clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"overall\": \"rating\",\n",
    "        \"reviewTime\": \"review_date\",\n",
    "        \"reviewerID\": \"reviewer_id\",\n",
    "        \"asin\": \"product_id\",\n",
    "        \"reviewText\": \"review_text\",\n",
    "    })\n",
    "\n",
    "    df[\"vote\"] = df[\"vote\"].fillna(0)\n",
    "    df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(\"str\").str.replace(\",\",\"\")).astype(\"int32\")\n",
    "    df = df[df[\"vote\"] >= 5]\n",
    "    df = filter_comment_length(df, 30)\n",
    "    df.drop([\"unixReviewTime\", \"image\", \"summary\", \"reviewerName\"], axis=1, inplace=True)\n",
    "    \n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], format=\"%m %d, %Y\")\n",
    "    df[\"style\"] = df[\"style\"].apply(extract_value)\n",
    "\n",
    "    df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
    "    df[\"review_text\"] = remove_irrelevant_info(df[\"review_text\"])\n",
    "    df[\"review_text\"] = reduce_characters(df[\"review_text\"])\n",
    "    df[\"review_text\"] = normalize_encoding(df[\"review_text\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcaa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(BOOKS_RAW_DIR)))]\n",
    "\n",
    "for raw_file_name in raw_file_names:\n",
    "    raw_df_list = []\n",
    "    raw_df = pd.read_json(os.path.join(BOOKS_RAW_DIR, raw_file_name), lines=True)\n",
    "   \n",
    "    raw_df = clean_chunk(raw_df)\n",
    "    print(f\"{raw_file_name} DIMENSIONS: {raw_df.shape}\")\n",
    "    clean_dir = os.path.join(BOOKS_CLEAN_DIR, raw_file_name)\n",
    "    raw_df.to_json(clean_dir, orient=\"records\")\n",
    "\n",
    "    del raw_df\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eae861a-afe7-4a17-97a4-a1dea5fc2ee9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b721b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_text(column: pd.Series, \n",
    "               batch_size: int) -> list:\n",
    "    \n",
    "    batches = np.array_split(np.array(column.values), int(np.ceil(len(column)/ batch_size)))\n",
    "    batches = [batch.tolist() for batch in batches]\n",
    "    return batches\n",
    "\n",
    "def load_sentiment_model(model: str):\n",
    "\n",
    "    return TextClassifier.load(model)\n",
    "\n",
    "def is_float(value):\n",
    "      \n",
    "      try:\n",
    "        float(value)\n",
    "        return True\n",
    "      except ValueError:\n",
    "        return False\n",
    "\n",
    "def analyze_sentiment(model, \n",
    "                      batch: np.ndarray):\n",
    "    \n",
    "    score = []\n",
    "    for sentence in batch:\n",
    "        sentence = Sentence(sentence)\n",
    "        model.predict(sentence, verbose=False)\n",
    "        try:\n",
    "            process = re.sub(r\"\\(|\\)\",\"\",str(sentence.labels[0]))\n",
    "            number = [float(s) for s in  process.split() if is_float(s) is True]\n",
    "            if \"POSITIVE\" in process:\n",
    "                score.append(number[0])\n",
    "            elif \"NEGATIVE\" in process:\n",
    "                score.append(-number[0])\n",
    "        except IndexError:\n",
    "            print(sentence)\n",
    "            score.append(np.nan)\n",
    "    return score\n",
    "   \n",
    "def process_batches(models: list[str], \n",
    "                    batches: list):\n",
    "                    \n",
    "    models_loaded = [load_sentiment_model(model) for model in models]\n",
    "    print(\"[Starting process...]\")\n",
    "    sentiment_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers= NUM_CORES) as executor:\n",
    "        sentiment_tasks = [executor.submit(analyze_sentiment, model, batch) for model, batch in zip(models_loaded, batches)]\n",
    "        for completed_task in concurrent.futures.as_completed(sentiment_tasks):\n",
    "            result = completed_task.result()\n",
    "            sentiment_results.extend(result)\n",
    "            print(\"[Finished with a Batch]\")\n",
    "        return sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09444c36-567e-41fc-9bf6-4e82688ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(BOOKS_CLEAN_DIR)))]\n",
    "\n",
    "for clean_file_name in clean_file_names[4:5]:\n",
    "    clean_df = pd.read_json(os.path.join(BOOKS_CLEAN_DIR, clean_file_name), orient=\"columns\")\n",
    "    batch_size = math.ceil(len(clean_df)/NUM_CORES) \n",
    "       \n",
    "    batches = batch_text(clean_df[\"review_text\"], batch_size)\n",
    "    clean_df[\"sentiment_score\"] = process_batches([\"en-sentiment\"] * len(batches), batches)\n",
    "    sentiment_dir = os.path.join(BOOKS_SENTIMENT_DIR, clean_file_name)\n",
    "    clean_df.to_json(sentiment_dir, orient=\"records\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b39d037",
   "metadata": {},
   "source": [
    "## Neural Network Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0c4a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>verified</th>\n",
       "      <th>review_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>style</th>\n",
       "      <th>vote</th>\n",
       "      <th>review_len</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>1087776000000</td>\n",
       "      <td>A2NJO6YE954DBH</td>\n",
       "      <td>0001712799</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>6</td>\n",
       "      <td>298</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>982972800000</td>\n",
       "      <td>A1K1JW1C5CUSUZ</td>\n",
       "      <td>0001712799</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>23</td>\n",
       "      <td>431</td>\n",
       "      <td>0.9937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1089244800000</td>\n",
       "      <td>A1JS302JFHH9DJ</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>9</td>\n",
       "      <td>273</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1079049600000</td>\n",
       "      <td>A26QTCZG2XR3JH</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>15</td>\n",
       "      <td>277</td>\n",
       "      <td>-0.6053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>1071360000000</td>\n",
       "      <td>A36X9BU9JB8KCE</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>5</td>\n",
       "      <td>495</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  verified    review_date     reviewer_id  product_id       style   \n",
       "0       4     False  1087776000000  A2NJO6YE954DBH  0001712799   Hardcover  \\\n",
       "1       5     False   982972800000  A1K1JW1C5CUSUZ  0001712799   Hardcover   \n",
       "2       3     False  1089244800000  A1JS302JFHH9DJ  0002006448   Hardcover   \n",
       "3       5      True  1079049600000  A26QTCZG2XR3JH  0002006448   Hardcover   \n",
       "4       5     False  1071360000000  A36X9BU9JB8KCE  0002006448   Hardcover   \n",
       "\n",
       "   vote  review_len  sentiment_score  \n",
       "0     6         298           0.9999  \n",
       "1    23         431           0.9937  \n",
       "2     9         273           0.9999  \n",
       "3    15         277          -0.6053  \n",
       "4     5         495           0.9994  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for filename in os.listdir(BOOKS_SENTIMENT_DIR):\n",
    "    sentiment_df = pd.read_json(os.path.join(BOOKS_SENTIMENT_DIR, filename), orient=\"columns\")\n",
    "    sentiment_df.drop(columns=[\"review_text\"], inplace=True)\n",
    "    merged_df = pd.concat([merged_df, sentiment_df])\n",
    "    \n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02212d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range -1.0 to -0.5: 198373\n",
      "Range -0.5 to 0.0: 4\n",
      "Range 0.0 to 0.5: 0\n",
      "Range 0.5 to 1.0: 192154\n"
     ]
    }
   ],
   "source": [
    "ranges = [(-1.0000, -0.5000), (-0.5000, 0.0000), (0.0000, 0.5000), (0.5000, 1.0000)] # Define the ranges\n",
    "\n",
    "counts = {f\"{r[0]} to {r[1]}\": 0 for r in ranges} # Initialize a dictionary to store the counts\n",
    "\n",
    "# Count the values within each range\n",
    "for value in merged_df[\"sentiment_score\"]:\n",
    "    for r in ranges:\n",
    "        if r[0] <= value < r[1]:\n",
    "            counts[f\"{r[0]} to {r[1]}\"] += 1\n",
    "\n",
    "for r, count in counts.items():\n",
    "    print(f\"Range {r}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a748ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_transformation(data: np.ndarray, \n",
    "                            type: typing.Literal[\"uniform\",\"normal\"]) -> np.ndarray:\n",
    "    \n",
    "    qt = QuantileTransformer(output_distribution=type)\n",
    "    return  qt.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "334c3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dropna(inplace=True)\n",
    "sentiment_score = np.array(merged_df[\"sentiment_score\"]).reshape(-1,1) # Needs to be 2D to use with QuantileTransformer\n",
    "merged_df[\"sentiment_uq\"] = quantile_transformation(sentiment_score, \"uniform\") # Reshape into uniform distribution -> Everything will be equally weighted\n",
    "merged_df[\"sentiment_nq\"] = quantile_transformation(sentiment_score, \"normal\") # reshape into normal distribution -> will create bias towards average values -0.5 to 0.5\n",
    "\n",
    "merged_df[\"reviewer_index\"] = merged_df[\"reviewer_id\"].astype(\"category\").cat.codes\n",
    "merged_df[\"product_index\"] = merged_df[\"product_id\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fddf75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svds_dot(U: np.ndarray, \n",
    "             Sigma: np.ndarray, \n",
    "             VT: np.ndarray, \n",
    "             num_rows: int, \n",
    "             num_cols: int, \n",
    "             embedding_length: int, \n",
    "             chunk_size: int) -> np.ndarray:\n",
    "\n",
    "    pred_matrix = sp.lil_matrix((num_rows, num_cols)) # This reduces the memory needed to create a matrix with large dimensions\n",
    "\n",
    "    for i in range(0, num_rows, chunk_size):\n",
    "        for j in range(0, num_cols, chunk_size):\n",
    "            for k in range(0, embedding_length, chunk_size):\n",
    "                u_chunk = U[i:i+chunk_size, k:k+chunk_size]\n",
    "                sigma_chunk = Sigma[k:k+chunk_size]\n",
    "                vt_chunk = VT[k:k+chunk_size, j:j+chunk_size]\n",
    "=\n",
    "                pred_matrix[i:i+chunk_size, j:j+chunk_size] += np.einsum(\"ik,kk,kj->ij\", u_chunk, np.diag(sigma_chunk), vt_chunk)\n",
    "    return pred_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684b98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = sp.csr_matrix((merged_df[\"rating\"], \n",
    "                               (merged_df[\"reviewer_index\"], merged_df[\"product_index\"])), \n",
    "                               shape=(len(merged_df[\"reviewer_index\"]), len(merged_df[\"product_index\"])), \n",
    "                               dtype=np.float64)\n",
    "\n",
    "embedding_length = 64\n",
    "U, Sigma, VT = svds(sparse_matrix, k = embedding_length)\n",
    "\n",
    "num_rows, num_cols = sparse_matrix.shape\n",
    "chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c74f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_matrix = svds_dot(U, Sigma, VT, num_rows, num_cols, embedding_length, chunk_size)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "pred_matrix = scaler.fit_transform(pred_matrix)\n",
    "pred_matrix = pd.DataFrame(pred_matrix, index = sparse_matrix.index, columns = sparse_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pivot_value(x):\n",
    "    val = pred_matrix.loc[x[0],x[1]]\n",
    "    return val\n",
    "\n",
    "predicted_df = merged_df.copy()\n",
    "predicted_df[\"Pred_Rating\"] = predicted_df[['User-ID','Book-ID']].apply(find_pivot_value, axis = 1)\n",
    "\n",
    "predicted_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
