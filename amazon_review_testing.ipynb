{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175e7786-8e51-4e84-849b-9d00e5d4c92a",
   "metadata": {},
   "source": [
    "# Recommender System with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "951e16ec",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "983b6c02",
   "metadata": {},
   "source": [
    "### Packages & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc723e1-d85f-40b3-91fa-0e8f85395711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import typing\n",
    "\n",
    "import urllib.request\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import gzip\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9021e6d4",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83509cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.join(\"D:\" + os.sep, \"Code\", \"PYTHON\", \"Amazon_Recommender_System\")\n",
    "\n",
    "CODE_DIR = os.path.join(MAIN_DIR, \"Code\")\n",
    "\n",
    "ANALYSIS_DIR = os.path.join(MAIN_DIR, \"Analysis\")\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"Data\")\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"Raw\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_DIR, \"Clean\")\n",
    "\n",
    "BOOKS_RAW_DIR = os.path.join(RAW_DATA_DIR, \"Books\")\n",
    "BOOKS_CLEAN_DIR = os.path.join(CLEAN_DATA_DIR, \"Books\")\n",
    "BOOKS_SENTIMENT_DIR = os.path.join(BOOKS_CLEAN_DIR, \"Sentiment\")\n",
    "\n",
    "CHUNK_SIZE = 1000000\n",
    "\n",
    "NUM_CORES = math.ceil(mp.cpu_count()/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71a50b4e",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a6c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_list: list) -> None:\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def is_directory_empty(dir_path: str) -> bool:\n",
    "    return len(os.listdir(dir_path)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3231a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory([CODE_DIR, \n",
    "                  ANALYSIS_DIR,\n",
    "                  DATA_DIR, \n",
    "                  RAW_DATA_DIR, \n",
    "                  CLEAN_DATA_DIR,\n",
    "                  BOOKS_RAW_DIR,\n",
    "                  BOOKS_CLEAN_DIR,\n",
    "                  BOOKS_SENTIMENT_DIR,\n",
    "                  ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53f8033",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca015dd",
   "metadata": {},
   "source": [
    "### Loading Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc72a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More memory efficient version. Takes longer but is far more consistent than the previous version. \n",
    "def process_data(url: str, \n",
    "                 chunk_size: int, \n",
    "                 num_workers: int,\n",
    "                 output_dir: str) -> None:\n",
    "    \n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with gzip.open(response, \"rt\") as gz_file:\n",
    "            chunk = []\n",
    "            chunk_count = 0\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                for line in gz_file:\n",
    "                    chunk.append(line)\n",
    "\n",
    "                    if len(chunk) == chunk_size:\n",
    "                        executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "                        chunk = []\n",
    "                        chunk_count += 1\n",
    "\n",
    "                # Process the remaining lines in the last chunk\n",
    "                if chunk:\n",
    "                    executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "\n",
    "def process_chunk(chunk: list,\n",
    "                  filename: str) -> None:\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        file.writelines(chunk)\n",
    "    print(f\"Processed chunk: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "----- PROCESS_DATA -----\n",
    "GAME URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "BOOK URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "EXPERIMENTS FOR 27 MILLION\n",
    "----- THREAD CSV_FILE -----\n",
    "1. 100000 chunk -> 22min 26sec\n",
    "\n",
    "----- THREAD JSON_FILE RUNTIMES-----\n",
    "1. 100000 chunk -> 20min 55sec\n",
    "2. 100000 chunk -> 49min 1sec -> No idea why this happened on a fresh start\n",
    "3. 500000 chunk -> 18min 45sec\n",
    "4. 1000000 chunk -> 17min 5sec\n",
    "\n",
    "NEW STABLE IMPLEMENTATION\n",
    "1. 1000000 chunk -> 22min 22sec\n",
    "'''\n",
    "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "process_data(url, CHUNK_SIZE, NUM_CORES, BOOKS_RAW_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe328f04",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f706d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(dictionary: dict):\n",
    "    return dictionary.get(\"Format:\") if isinstance(dictionary, dict) else None\n",
    "\n",
    "def filter_comment_length(reviews: pd.DataFrame, \n",
    "                          minimum: int) -> pd.DataFrame:\n",
    "    reviews = reviews.copy()\n",
    "    reviews[\"review_len\"] = reviews[\"review_text\"].str.split().str.len()\n",
    "    reviews = reviews.loc[(reviews[\"review_len\"] > minimum)]\n",
    "    return reviews\n",
    "\n",
    "def remove_symbols(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"\\s+|[^a-zA-Z0-9\\s]\", \"\", x)) # Cleans up duplicate space and special characters.\n",
    "\n",
    "def remove_irrelevant_info(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"http\\S+|www.\\S+|#\\S+|<.*?>|\\(|\\)|\\d+\", \"\", x)) # Cleans up URL, hashtags, parenthesis, and numbers.\n",
    "\n",
    "def reduce_characters(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"[^\\w\\s]|(.)\\1+\", \"\", x)) # Remove excessive punctuation and repeated characters\n",
    "\n",
    "def normalize_encoding(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")) # Encodes and Decodes the data so that we have consistency in text\n",
    "        \n",
    "def clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"overall\": \"rating\",\n",
    "        \"reviewTime\": \"review_date\",\n",
    "        \"reviewerID\": \"reviewer_id\",\n",
    "        \"asin\": \"product_id\",\n",
    "        \"reviewText\": \"review_text\",\n",
    "    })\n",
    "\n",
    "    df[\"vote\"] = df[\"vote\"].fillna(0)\n",
    "    df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(\"str\").str.replace(\",\",\"\")).astype(\"int32\")\n",
    "    df = df[df[\"vote\"] >= 5]\n",
    "    df = filter_comment_length(df, 30)\n",
    "    df.drop([\"unixReviewTime\", \"image\", \"summary\", \"reviewerName\"], axis=1, inplace=True)\n",
    "    \n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], format=\"%m %d, %Y\")\n",
    "    df[\"style\"] = df[\"style\"].apply(extract_value)\n",
    "\n",
    "    df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
    "    df[\"review_text\"] = remove_irrelevant_info(df[\"review_text\"])\n",
    "    df[\"review_text\"] = reduce_characters(df[\"review_text\"])\n",
    "    df[\"review_text\"] = normalize_encoding(df[\"review_text\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcaa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(BOOKS_RAW_DIR)))]\n",
    "\n",
    "for raw_file_name in raw_file_names:\n",
    "    raw_df_list = []\n",
    "    raw_df = pd.read_json(os.path.join(BOOKS_RAW_DIR, raw_file_name), lines=True)\n",
    "   \n",
    "    raw_df = clean_chunk(raw_df)\n",
    "    print(f\"{raw_file_name} DIMENSIONS: {raw_df.shape}\")\n",
    "    clean_dir = os.path.join(BOOKS_CLEAN_DIR, raw_file_name)\n",
    "    raw_df.to_json(clean_dir, orient=\"records\")\n",
    "\n",
    "    del raw_df\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eae861a-afe7-4a17-97a4-a1dea5fc2ee9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b721b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_text(column: pd.Series, \n",
    "               batch_size: int) -> list:\n",
    "    \n",
    "    batches = np.array_split(np.array(column.values), int(np.ceil(len(column)/ batch_size)))\n",
    "    batches = [batch.tolist() for batch in batches]\n",
    "    return batches\n",
    "\n",
    "def load_sentiment_model(model: str):\n",
    "\n",
    "    return TextClassifier.load(model)\n",
    "\n",
    "def is_float(value):\n",
    "      \n",
    "      try:\n",
    "        float(value)\n",
    "        return True\n",
    "      except ValueError:\n",
    "        return False\n",
    "\n",
    "def analyze_sentiment(model, \n",
    "                      batch: np.ndarray):\n",
    "    \n",
    "    score = []\n",
    "    for sentence in batch:\n",
    "        sentence = Sentence(sentence)\n",
    "        model.predict(sentence, verbose=False)\n",
    "        try:\n",
    "            process = re.sub(r\"\\(|\\)\",\"\",str(sentence.labels[0]))\n",
    "            number = [float(s) for s in  process.split() if is_float(s) is True]\n",
    "            if \"POSITIVE\" in process:\n",
    "                score.append(number[0])\n",
    "            elif \"NEGATIVE\" in process:\n",
    "                score.append(-number[0])\n",
    "        except IndexError:\n",
    "            print(sentence)\n",
    "            score.append(np.nan)\n",
    "    return score\n",
    "   \n",
    "def process_batches(models: list[str], \n",
    "                    batches: list):\n",
    "                    \n",
    "    models_loaded = [load_sentiment_model(model) for model in models]\n",
    "    print(\"[Starting process...]\")\n",
    "    sentiment_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers= NUM_CORES) as executor:\n",
    "        sentiment_tasks = [executor.submit(analyze_sentiment, model, batch) for model, batch in zip(models_loaded, batches)]\n",
    "        for completed_task in concurrent.futures.as_completed(sentiment_tasks):\n",
    "            result = completed_task.result()\n",
    "            sentiment_results.extend(result)\n",
    "            print(\"[Finished with a Batch]\")\n",
    "        return sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09444c36-567e-41fc-9bf6-4e82688ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(BOOKS_CLEAN_DIR)))]\n",
    "\n",
    "for clean_file_name in clean_file_names[4:5]:\n",
    "    clean_df = pd.read_json(os.path.join(BOOKS_CLEAN_DIR, clean_file_name), orient=\"columns\")\n",
    "    batch_size = math.ceil(len(clean_df)/NUM_CORES) \n",
    "       \n",
    "    batches = batch_text(clean_df[\"review_text\"], batch_size)\n",
    "    clean_df[\"sentiment_score\"] = process_batches([\"en-sentiment\"] * len(batches), batches)\n",
    "    sentiment_dir = os.path.join(BOOKS_SENTIMENT_DIR, clean_file_name)\n",
    "    clean_df.to_json(sentiment_dir, orient=\"records\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b39d037",
   "metadata": {},
   "source": [
    "## Neural Network Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0c4a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>verified</th>\n",
       "      <th>review_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>style</th>\n",
       "      <th>vote</th>\n",
       "      <th>review_len</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>1087776000000</td>\n",
       "      <td>A2NJO6YE954DBH</td>\n",
       "      <td>0001712799</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>6</td>\n",
       "      <td>298</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>982972800000</td>\n",
       "      <td>A1K1JW1C5CUSUZ</td>\n",
       "      <td>0001712799</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>23</td>\n",
       "      <td>431</td>\n",
       "      <td>0.9937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1089244800000</td>\n",
       "      <td>A1JS302JFHH9DJ</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>9</td>\n",
       "      <td>273</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1079049600000</td>\n",
       "      <td>A26QTCZG2XR3JH</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>15</td>\n",
       "      <td>277</td>\n",
       "      <td>-0.6053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>1071360000000</td>\n",
       "      <td>A36X9BU9JB8KCE</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>5</td>\n",
       "      <td>495</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  verified    review_date     reviewer_id  product_id       style   \n",
       "0       4     False  1087776000000  A2NJO6YE954DBH  0001712799   Hardcover  \\\n",
       "1       5     False   982972800000  A1K1JW1C5CUSUZ  0001712799   Hardcover   \n",
       "2       3     False  1089244800000  A1JS302JFHH9DJ  0002006448   Hardcover   \n",
       "3       5      True  1079049600000  A26QTCZG2XR3JH  0002006448   Hardcover   \n",
       "4       5     False  1071360000000  A36X9BU9JB8KCE  0002006448   Hardcover   \n",
       "\n",
       "   vote  review_len  sentiment_score  \n",
       "0     6         298           0.9999  \n",
       "1    23         431           0.9937  \n",
       "2     9         273           0.9999  \n",
       "3    15         277          -0.6053  \n",
       "4     5         495           0.9994  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for filename in os.listdir(BOOKS_SENTIMENT_DIR):\n",
    "    sentiment_df = pd.read_json(os.path.join(BOOKS_SENTIMENT_DIR, filename), orient=\"columns\")\n",
    "    sentiment_df.drop(columns=[\"review_text\"], inplace=True)\n",
    "    merged_df = pd.concat([merged_df, sentiment_df])\n",
    "    \n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02212d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range -1.0 to -0.5: 198373\n",
      "Range -0.5 to 0.0: 4\n",
      "Range 0.0 to 0.5: 0\n",
      "Range 0.5 to 1.0: 192154\n"
     ]
    }
   ],
   "source": [
    "ranges = [(-1.0000, -0.5000), (-0.5000, 0.0000), (0.0000, 0.5000), (0.5000, 1.0000)] # Define the ranges\n",
    "\n",
    "counts = {f\"{r[0]} to {r[1]}\": 0 for r in ranges} # Initialize a dictionary to store the counts\n",
    "\n",
    "# Count the values within each range\n",
    "for value in merged_df[\"sentiment_score\"]:\n",
    "    for r in ranges:\n",
    "        if r[0] <= value < r[1]:\n",
    "            counts[f\"{r[0]} to {r[1]}\"] += 1\n",
    "\n",
    "for r, count in counts.items():\n",
    "    print(f\"Range {r}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a748ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_transformation(data: np.ndarray, \n",
    "                            type: typing.Literal[\"uniform\",\"normal\"]) -> np.ndarray:\n",
    "    \n",
    "    qt = QuantileTransformer(output_distribution=type)\n",
    "    return  qt.fit_transform(data)\n",
    "\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "def create_dataset(df:pd.DataFrame, ratings_column: str):\n",
    "    unique_users = df[\"reviewer_id\"].unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = df[\"reviewer_id\"].map(user_to_index)\n",
    "\n",
    "    unique_items = df[\"product_id\"].unique()\n",
    "    item_to_index = {old: new for new, old in enumerate(unique_items)}\n",
    "    new_items = df[\"product_id\"].map(item_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_items = unique_items.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({\"user_id\": new_users, \"item_id\": new_items})\n",
    "    y = df[ratings_column].astype(np.float32)\n",
    "    return (n_users, n_items), (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334c3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 165643 users, 50565 items\n",
      "Dataset shape: (389012, 2)\n",
      "Target shape: (389012,)\n"
     ]
    }
   ],
   "source": [
    "sentiment_score = np.array(merged_df[\"sentiment_score\"]).reshape(-1,1) # Needs to be 2D to use with QuantileTransformer\n",
    "merged_df[\"sentiment_uq\"] = quantile_transformation(sentiment_score, \"uniform\") # Reshape into uniform distribution -> Everything will be equally weighted\n",
    "merged_df[\"sentiment_nq\"] = quantile_transformation(sentiment_score, \"normal\") # reshape into normal distribution -> will create bias towards average values -0.5 to 0.5\n",
    "\n",
    "(n_users, n_items), (X, y) = create_dataset(merged_df, \"sentiment_score\")\n",
    "print(f\"Embeddings: {n_users} users, {n_items} items\")\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def svds_dot(U: np.ndarray, \n",
    "#              Sigma: np.ndarray, \n",
    "#              VT: np.ndarray, \n",
    "#              num_rows: int, \n",
    "#              num_cols: int, \n",
    "#              embedding_length: int, \n",
    "#              chunk_size: int) -> np.ndarray:\n",
    "\n",
    "#     pred_matrix = sp.lil_matrix((num_rows, num_cols)) # This reduces the memory needed to create a matrix with large dimensions\n",
    "\n",
    "#     for i in range(0, num_rows, chunk_size):\n",
    "#         for j in range(0, num_cols, chunk_size):\n",
    "#             for k in range(0, embedding_length, chunk_size):\n",
    "#                 u_chunk = U[i:i+chunk_size, k:k+chunk_size]\n",
    "#                 sigma_chunk = Sigma[k:k+chunk_size]\n",
    "#                 vt_chunk = VT[k:k+chunk_size, j:j+chunk_size]\n",
    "\n",
    "#                 pred_matrix[i:i+chunk_size, j:j+chunk_size] += np.einsum(\"ik,kk,kj->ij\", u_chunk, np.diag(sigma_chunk), vt_chunk)\n",
    "#     return pred_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_matrix = sp.csr_matrix((merged_df[\"rating\"], \n",
    "#                                (merged_df[\"reviewer_index\"], merged_df[\"product_index\"])), \n",
    "#                                shape=(len(merged_df[\"reviewer_index\"]), len(merged_df[\"product_index\"])), \n",
    "#                                dtype=np.float64)\n",
    "\n",
    "# embedding_length = 64\n",
    "# U, Sigma, VT = svds(sparse_matrix, k = embedding_length)\n",
    "\n",
    "# num_rows, num_cols = sparse_matrix.shape\n",
    "# chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_matrix = svds_dot(U, Sigma, VT, num_rows, num_cols, embedding_length, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5131589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# row_indices, column_indices = sparse_matrix.nonzero()\n",
    "# pred_matrix = scaler.fit_transform(pred_matrix)\n",
    "# pred_matrix = pd.DataFrame(pred_matrix, index = row_indices, columns = column_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "        def __init__(self,\n",
    "                     n_users,\n",
    "                     n_items,\n",
    "                     n_factors=40,\n",
    "                     dropout_p=0.02,\n",
    "                     sparse=False):\n",
    "                \n",
    "                super(MatrixFactorization, self).__init__()\n",
    "                self.n_users = n_users\n",
    "                self.n_items = n_items\n",
    "                self.n_factors = n_factors\n",
    "\n",
    "                self.user_biases = nn.Embedding(n_users, 1, sparse=sparse)\n",
    "                self.item_biases = nn.Embedding(n_items, 1, sparse=sparse)\n",
    "                self.user_embeddings = nn.Embedding(n_users, n_factors, sparse=sparse)\n",
    "                self.item_embeddings = nn.Embedding(n_items, n_factors, sparse=sparse)\n",
    "\n",
    "                self.dropout_p = dropout_p\n",
    "                self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "                self.sparse = sparse\n",
    "\n",
    "        def forward(self, users, items):\n",
    "                user_embedding = self.user_embeddings(users)\n",
    "                item_embedding = self.item_embeddings(items)\n",
    "                predictions = self.user_biases(users)\n",
    "                predictions += self.item_biases(items)\n",
    "                predictions += (self.dropout(user_embedding) * self.dropout(item_embedding)).sum(dim=1, keepdim=True)\n",
    "                return predictions\n",
    "        \n",
    "        def __call__(self, *args):\n",
    "                return self.forward(*args)\n",
    "        \n",
    "        def predict(self, users, items):\n",
    "                return self.forward(users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 22)\n",
    "merged_df \n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors = 64)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=e26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Network as defined in Zhou Xu 2016\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_factors=64,\n",
    "                 hidden_dim,\n",
    "                 dropout_p=20,\n",
    "                 sparse=False,\n",
    "                 output_dim):\n",
    "        \n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "        self.user_biases = nn.Embedding(n_users, 1, sparse=sparse)\n",
    "        self.item_biases = nn.Embedding(n_items, 1, sparse=sparse)\n",
    "        self.user_embeddings = nn.Embedding(n_users, n_factors, sparse=sparse)\n",
    "        self.item_embeddings = nn.Embedding(n_items, n_factors, sparse=sparse)\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "        self.sparse=sparse\n",
    "\n",
    "        # Define the layers\n",
    "        self.linear1 = nn.Linear(n_factors*2, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.linear4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.linear6 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        users_embedding = self.user_embeddings(users)\n",
    "        items_embedding = self.item_embeddings(items)\n",
    "\t\n",
    "        x = torch.cat([users_embedding, items_embedding], 1) # concatenate user and item embeddings to form input\n",
    "       \n",
    "        h1 = self.bn1(self.dropout1(F.relu(self.linear1(x))))  # Layer 1: ReLU(W(1)x + b1)\n",
    "\n",
    "        h2 = self.bn2(self.dropout2(torch.tanh(self.linear2(h1)))) # Layer 2: tanh(W(2)h(1) + b2)\n",
    "\n",
    "        h3 = self.bn3(self.dropout3(F.relu(self.linear3(h2)))) # Layer 3: ReLU(W(3)h(2) + b3)\n",
    "\n",
    "        h4 = self.bn4(self.dropout4(torch.sigmoid(self.linear4(h3)))) # Layer 4: Sigmoid(W(4)h(3) + b4)\n",
    "\n",
    "        h5 = self.bn5(self.dropout5(F.relu(self.linear5(h4)))) # Layer 5: ReLU(W(5)h(4) + b5)\n",
    "\n",
    "        output = F.softmax(self.linear6(h5), dim=1) # Output layer: softmax(Uh(5) + b6)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd60b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM as defined in Zhou Xu 2016\n",
    "class LSTM_Rating(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 n_users, \n",
    "                 n_items, \n",
    "                 n_factors = 64, \n",
    "                 n_output, \n",
    "                 sparse):\n",
    "        \n",
    "        super(LSTM_Rating, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "        self.user_biases = nn.Embedding(n_users, 1, sparse=sparse)\n",
    "        self.item_biases = nn.Embedding(n_items, 1, sparse=sparse)\n",
    "        self.user_embeddings = nn.Embedding(n_users, n_factors, sparse=sparse)\n",
    "        self.item_embeddings = nn.Embedding(n_items, n_factors, sparse=sparse)\n",
    "\n",
    "        self.sparse = sparse\n",
    "        # Input gate\n",
    "        self.Wu = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Iu = nn.Linear(input_size, hidden_dim)\n",
    "\n",
    "        # Forget gate\n",
    "        self.Wf = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.If = nn.Linear(input_size, hidden_dim)\n",
    "\n",
    "        # Output gate\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Io = nn.Linear(input_size, hidden_dim)\n",
    "\n",
    "        # New memory cell\n",
    "        self.Wc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Ic = nn.Linear(input_size, hidden_dim)\n",
    "\n",
    "    def forward(self, xt, ht_1, ct_1):\n",
    "    \n",
    "        gu = torch.sigmoid(self.Wu(ht_1) + self.Iu(xt)) # Input gate\n",
    "\n",
    "        gf = torch.sigmoid(self.Wf(ht_1) + self.If(xt)) # Forget gate\n",
    "\n",
    "        go = torch.sigmoid(self.Wo(ht_1) + self.Io(xt)) # Output gate\n",
    "\n",
    "        gc = torch.tanh(self.Wc(ht_1) + self.Ic(xt)) # New memory cell\n",
    "\n",
    "        ct = gf * ct_1 + gu * gc  # Final memory cell\n",
    "\n",
    "        ht = torch.tanh(go * ct) # Final hidden state\n",
    "\n",
    "        return ht, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n",
    "    x = torch.dist(A, U @ torch.diag(S) @ Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e327a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
