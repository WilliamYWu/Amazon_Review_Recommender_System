{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, json, typing\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Generator\n",
    "import urllib.request\n",
    "import ssl\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Disable the certificate verification process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(line: str) -> dict:\n",
    "    return json.loads(line)\n",
    "\n",
    "def read_gz_file(url: str, chunk_size: int) -> list:\n",
    "    response = urllib.request.urlopen(url)\n",
    "    with gzip.open(response, \"rt\") as gz_file:\n",
    "        for i, line in enumerate(gz_file):\n",
    "            # Using generator objects we pass each item into the list rather than load everything into the memory at once\n",
    "            # This lets us avoid the memory storage of creating a temporary list and appending each line to each list\n",
    "            if (i + 1) % chunk_size == 1:\n",
    "                chunk = (l for l in [line]) # If chunk_size is 1 then a new chunk is created containing only the current line\n",
    "            else:                                       \n",
    "                chunk = (l for l in [*chunk, line]) # Any other index will result in items being added to the existing list\n",
    "                \n",
    "            if (i + 1) % chunk_size == 0: # If we have the perfect chunk size then they are then yielded\n",
    "                yield chunk\n",
    "                \n",
    "        if chunk:\n",
    "            yield chunk # If for some reason we have leftover lines that don't make up a full chunk we yield those as well\n",
    "            \n",
    "def process_chunk(chunk: Generator[str, None, None], file_name: str) -> None:             \n",
    "    records = (parse(line) for line in chunk)\n",
    "    table = pa.Table.from_pydict({\"col\": list(records)})\n",
    "    pq.write_table(table, file_name)\n",
    "    \n",
    "def process_data(url: str, chunk_size: int, num_workers: int, output_dir: str) -> None:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for i, chunk in enumerate(read_gz_file(url, chunk_size)):\n",
    "            file_name = os.path.join(output_dir, f\"chunk_{i}.parquet\")\n",
    "            executor.submit(process_chunk, chunk, file_name)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url: str):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    gz_file = gzip.open(response, \"r\")\n",
    "    for line in gz_file:\n",
    "        yield json.loads(line)\n",
    "\n",
    "def toDF(url: str) -> pd.DataFrame:\n",
    "    index = 0\n",
    "    df = {}\n",
    "    for line in parse(url):\n",
    "        df[index] = line\n",
    "        index += 1\n",
    "    return pd.DataFrame.from_dict(df, orient=\"index\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster Probably, but will take more memory\n",
    "# Also pretty inconsistent since it each thread is trying to process the chunks at once.\n",
    "# Breaks when I try to optimize\n",
    "def read_gz_file(url: str) -> list:\n",
    "    with gzip.open(urllib.request.urlopen(url), \"rt\") as gz_file:\n",
    "        lines = gz_file.readlines()\n",
    "    return lines\n",
    "\n",
    "def write_lines(lines: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as file:\n",
    "        for line in lines:\n",
    "            file.write(line)\n",
    "    \n",
    "def process_chunk(chunk: list, filename: str) -> None:\n",
    "    write_lines(chunk, filename)\n",
    "    print(f\"FINISHED CHUNK: {filename}\")\n",
    "\n",
    "def process_data(url: str, chunk_size: int, num_workers: int, output_dir: str) -> None:\n",
    "    lines = read_gz_file(url)\n",
    "    chunks = [lines[i:i+chunk_size] for i in range(0,len(lines), chunk_size)]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            future = executor.submit(process_chunk, chunk, os.path.join(output_dir, f\"chunk_{i}.json\"))\n",
    "            futures.append(future)\n",
    "        concurrent.futures.wait(futures)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
