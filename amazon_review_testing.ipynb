{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175e7786-8e51-4e84-849b-9d00e5d4c92a",
   "metadata": {},
   "source": [
    "# Recommender System with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "951e16ec",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "983b6c02",
   "metadata": {},
   "source": [
    "### Packages & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc723e1-d85f-40b3-91fa-0e8f85395711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gzip, sys, gc\n",
    "import re\n",
    "\n",
    "import urllib.request\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9021e6d4",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83509cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.join(\"D:\" + os.sep, \"Code\", \"PYTHON\", \"Amazon_Recommender_System\")\n",
    "\n",
    "CODE_DIR = os.path.join(MAIN_DIR, \"Code\")\n",
    "\n",
    "ANALYSIS_DIR = os.path.join(MAIN_DIR, \"Analysis\")\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"Data\")\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"Raw\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_DIR, \"Clean\")\n",
    "\n",
    "VIDEO_GAME_RAW_DIR = os.path.join(RAW_DATA_DIR, \"Video_Game\")\n",
    "VIDEO_GAME_CLEAN_DIR = os.path.join(CLEAN_DATA_DIR, \"Video_Game\")\n",
    "BOOKS_RAW_DIR = os.path.join(RAW_DATA_DIR, \"Books\")\n",
    "BOOKS_CLEAN_DIR = os.path.join(CLEAN_DATA_DIR, \"Books\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71a50b4e",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a6c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_list: list) -> None:\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def is_directory_empty(dir_path: str) -> bool:\n",
    "    return len(os.listdir(dir_path)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3231a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory([CODE_DIR, \n",
    "                  ANALYSIS_DIR,\n",
    "                  DATA_DIR, \n",
    "                  RAW_DATA_DIR, \n",
    "                  CLEAN_DATA_DIR,\n",
    "                  VIDEO_GAME_RAW_DIR,\n",
    "                  VIDEO_GAME_CLEAN_DIR,\n",
    "                  BOOKS_RAW_DIR,\n",
    "                  BOOKS_CLEAN_DIR])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53f8033",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca015dd",
   "metadata": {},
   "source": [
    "### Loading Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc72a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More memory efficient version. Takes longer but is far more consistent than the previous version. \n",
    "def process_data(url: str, chunk_size: int, num_workers: int, output_dir: str) -> None:\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with gzip.open(response, \"rt\") as gz_file:\n",
    "            chunk = []\n",
    "            chunk_count = 0\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                for line in gz_file:\n",
    "                    chunk.append(line)\n",
    "\n",
    "                    if len(chunk) == chunk_size:\n",
    "                        executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "                        chunk = []\n",
    "                        chunk_count += 1\n",
    "\n",
    "                # Process the remaining lines in the last chunk\n",
    "                if chunk:\n",
    "                    executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "\n",
    "def process_chunk(chunk: list, filename: str) -> None:\n",
    "    with open(filename, 'w') as file:\n",
    "        file.writelines(chunk)\n",
    "    print(f\"Processed chunk: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "----- PROCESS_DATA -----\n",
    "GAME URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "BOOK URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "EXPERIMENTS FOR 27 MILLION\n",
    "----- THREAD CSV_FILE -----\n",
    "1. 100000 chunk -> 22min 26sec\n",
    "\n",
    "----- THREAD JSON_FILE RUNTIMES-----\n",
    "1. 100000 chunk -> 20min 55sec\n",
    "2. 100000 chunk -> 49min 1sec -> No idea why this happened on a fresh start\n",
    "3. 500000 chunk -> 18min 45sec\n",
    "4. 1000000 chunk -> 17min 5sec\n",
    "\n",
    "NEW STABLE IMPLEMENTATION\n",
    "1. 1000000 chunk -> 22min 22sec\n",
    "'''\n",
    "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "url_data_directory = BOOKS_RAW_DIR\n",
    "chunk_size = 1000000\n",
    "num_cores = int(mp.cpu_count()/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f951cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(url, chunk_size, num_cores, url_data_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe328f04",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f706d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(dictionary: dict):\n",
    "    return dictionary.get(\"Format:\") if isinstance(dictionary, dict) else None\n",
    "\n",
    "def filter_comment_length(reviews: pd.DataFrame, minimum: int) -> pd.DataFrame:\n",
    "    reviews = reviews.copy()\n",
    "    reviews[\"review_len\"] = reviews[\"review_text\"].str.split().str.len()\n",
    "    reviews = reviews.loc[(reviews[\"review_len\"] > minimum)]\n",
    "    return reviews\n",
    "\n",
    "def remove_non_text(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "\n",
    "def tokenize_text(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: [token for token in x if token not in stop_words])\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: [stemmer.stem(token) for token in x])\n",
    "        \n",
    "def clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"vote\"] = df[\"vote\"].fillna(0)\n",
    "    df[\"vote\"] = pd.to_numeric(df['vote'].astype(\"str\").str.replace(\",\",\"\")).astype(\"int32\")\n",
    "    df  = df[df[\"vote\"] > 5]\n",
    "    df = filter_comment_length(df, 100)\n",
    "    df = df.drop([\"unixReviewTime\", \"image\", \"summary\", \"reviewerName\"], axis=1)\n",
    "    \n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], format=\"%m %d, %Y\")\n",
    "    df[\"style\"] = df[\"style\"].apply(extract_value)\n",
    "\n",
    "    df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
    "    df[\"review_text\"] = remove_non_text(df[\"review_text\"])\n",
    "    df[\"review_text\"] = tokenize_text(df[\"review_text\"])\n",
    "    df[\"review_text\"] = remove_stopwords(df[\"review_text\"])\n",
    "    df[\"review_text\"] = stemming(df[\"review_text\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdcaa284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_23.json DIMENSIONS: (36238, 9)\n",
      "chunk_24.json DIMENSIONS: (40533, 9)\n",
      "chunk_25.json DIMENSIONS: (16429, 9)\n",
      "chunk_26.json DIMENSIONS: (25489, 9)\n",
      "chunk_27.json DIMENSIONS: (844, 9)\n"
     ]
    }
   ],
   "source": [
    "file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(url_data_directory)))]\n",
    "for file_name in file_names:\n",
    "    raw_df_list = []\n",
    "    raw_df = pd.read_json(os.path.join(url_data_directory, file_name), lines=True)\n",
    "\n",
    "    raw_df = raw_df.rename(columns={\n",
    "        \"overall\": \"rating\",\n",
    "        \"reviewTime\": \"review_date\",\n",
    "        \"reviewerID\": \"reviewer_id\",\n",
    "        \"asin\": \"product_id\",\n",
    "        \"reviewText\": \"review_text\",\n",
    "    })\n",
    "\n",
    "    raw_df = clean_chunk(raw_df)\n",
    "    print(f\"{file_name} DIMENSIONS: {raw_df.shape}\")\n",
    "    clean_url = os.path.join(url_data_directory, file_name).replace(\"Raw\", \"Clean\")\n",
    "    raw_df.to_json(clean_url, orient=\"records\")\n",
    "    del raw_df\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eae861a-afe7-4a17-97a4-a1dea5fc2ee9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3c393-12b3-4412-904e-6d239342060e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_df = df[[\"reviewerID\", \"asin\", \"reviewText\", \"overall\"]].copy()\n",
    "filter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f1a30-fa98-4ee8-b426-23b9afcf8f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function takes a dataset in and them filters the users and products that are above a threshold\n",
    "Returns a cleaned dataframe of User & Product IDs with their Ratings\n",
    "'''\n",
    "def user_product_threshold_reviews(df, user_threshold, product_threshold):\n",
    "    product_count = df.groupby(by=[\"asin\"])[\"overall\"].count().reset_index().rename(columns={\"overall\":\"product_count\"})\n",
    "    product_count = product_count.query(\"product_count >= @product_threshold\")\n",
    "    \n",
    "    user_count = df.groupby(by=[\"reviewerID\"])[\"overall\"].count().reset_index().rename(columns={\"overall\":\"user_count\"})\n",
    "    user_count = user_count.query(\"user_count >= @user_threshold\")\n",
    "    \n",
    "    combined_df = pd.merge(product_count, sa_df, left_on=\"asin\", right_on=\"asin\", how=\"left\").merge(user_count, left_on=\"reviewerID\", right_on=\"reviewerID\", how=\"inner\")\n",
    "    \n",
    "    combined_gb = combined_df.groupby(by = [\"reviewerID\",\"asin\", \"reviewText\"],as_index=False).mean()\n",
    "\n",
    "    combined_final = combined_gb.rename(columns = {\"reviewerID\":\"User_ID\", \"asin\": \"Product_ID\", \"overall\":\"Rating\"})\n",
    "    # scaler = MinMaxScaler()\n",
    "    # combined_final['Rating'] = combined_final['Rating'].values.astype(float)\n",
    "    # rating_scaled = pd.DataFrame(scaler.fit_transform(combined_final['Rating'].values.reshape(-1,1)))\n",
    "    # combined_final['Rating'] = rating_scaled\n",
    "\n",
    "    return combined_final\n",
    "\n",
    "sentiment_df = user_product_threshold_reviews(filter_df, 100, 100)\n",
    "print(\"Number of Unique Product\", sentiment_df[\"Product_ID\"].nunique())\n",
    "print(\"Number of Unique Users\", sentiment_df[\"User_ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9908ff-afc7-4cae-bf25-638b88c7dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import re\n",
    "def flair_sentiment(df):\n",
    "    def isfloat(value):\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "        sia = TextClassifier.load(\"en-sentiment\")\n",
    "        \n",
    "        def flair_prediction(x):\n",
    "            sentence = Sentence(x)\n",
    "            sia.predict(sentence)\n",
    "            score = str(sentence.labels[0])\n",
    "            score = score.replace('(',' ').replace(')',' ')\n",
    "            number = [float(s) for s in score.split() if isfloat(s) is True]\n",
    "            \n",
    "            if \"POSITIVE\" in score:\n",
    "                return number[0]\n",
    "            elif \"NEGATIVE\" in score:\n",
    "                return -number[0]\n",
    "        \n",
    "    df[\"flair_sentiment\"] = df[\"reviewText\"].apply(flair_prediction)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24d0cc-7a8a-4fb6-b713-5da45fbd62d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews_list = np.array_split(reviews_filtered,100)\n",
    "\n",
    "flair_list = []\n",
    "for i in range (0, len(reviews_list)):\n",
    "    flair_review = flair_sentiment(reviews_list[i])\n",
    "    flair_list.append(flair_review)\n",
    "    \n",
    "reviews_filtered = pd.concat(flair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f8e24-6904-4f60-ac7e-0d92d063bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filtered = pd.read_csv('reviews_filtered_100_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09444c36-567e-41fc-9bf6-4e82688ae8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
