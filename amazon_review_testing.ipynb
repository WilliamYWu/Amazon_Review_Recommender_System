{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175e7786-8e51-4e84-849b-9d00e5d4c92a",
   "metadata": {},
   "source": [
    "# Recommender System with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "951e16ec",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "983b6c02",
   "metadata": {},
   "source": [
    "### Packages & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc723e1-d85f-40b3-91fa-0e8f85395711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import urllib.request\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import gzip\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9021e6d4",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83509cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.join(\"D:\" + os.sep, \"Code\", \"PYTHON\", \"Amazon_Recommender_System\")\n",
    "\n",
    "CODE_DIR = os.path.join(MAIN_DIR, \"Code\")\n",
    "\n",
    "ANALYSIS_DIR = os.path.join(MAIN_DIR, \"Analysis\")\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"Data\")\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"Raw\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_DIR, \"Clean\")\n",
    "\n",
    "BOOKS_RAW_DIR = os.path.join(RAW_DATA_DIR, \"Books\")\n",
    "BOOKS_CLEAN_DIR = os.path.join(CLEAN_DATA_DIR, \"Books\")\n",
    "BOOKS_SENTIMENT_DIR = os.path.join(BOOKS_CLEAN_DIR, \"Sentiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71a50b4e",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7a6c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_list: list) -> None:\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def is_directory_empty(dir_path: str) -> bool:\n",
    "    return len(os.listdir(dir_path)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3231a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory([CODE_DIR, \n",
    "                  ANALYSIS_DIR,\n",
    "                  DATA_DIR, \n",
    "                  RAW_DATA_DIR, \n",
    "                  CLEAN_DATA_DIR,\n",
    "                  BOOKS_RAW_DIR,\n",
    "                  BOOKS_CLEAN_DIR,\n",
    "                  BOOKS_SENTIMENT_DIR,\n",
    "                  ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53f8033",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca015dd",
   "metadata": {},
   "source": [
    "### Loading Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc72a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More memory efficient version. Takes longer but is far more consistent than the previous version. \n",
    "def process_data(url: str, chunk_size: int, num_workers: int, output_dir: str) -> None:\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with gzip.open(response, \"rt\") as gz_file:\n",
    "            chunk = []\n",
    "            chunk_count = 0\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                for line in gz_file:\n",
    "                    chunk.append(line)\n",
    "\n",
    "                    if len(chunk) == chunk_size:\n",
    "                        executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "                        chunk = []\n",
    "                        chunk_count += 1\n",
    "\n",
    "                # Process the remaining lines in the last chunk\n",
    "                if chunk:\n",
    "                    executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "\n",
    "def process_chunk(chunk: list, filename: str) -> None:\n",
    "    with open(filename, 'w') as file:\n",
    "        file.writelines(chunk)\n",
    "    print(f\"Processed chunk: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "099c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "----- PROCESS_DATA -----\n",
    "GAME URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "BOOK URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "EXPERIMENTS FOR 27 MILLION\n",
    "----- THREAD CSV_FILE -----\n",
    "1. 100000 chunk -> 22min 26sec\n",
    "\n",
    "----- THREAD JSON_FILE RUNTIMES-----\n",
    "1. 100000 chunk -> 20min 55sec\n",
    "2. 100000 chunk -> 49min 1sec -> No idea why this happened on a fresh start\n",
    "3. 500000 chunk -> 18min 45sec\n",
    "4. 1000000 chunk -> 17min 5sec\n",
    "\n",
    "NEW STABLE IMPLEMENTATION\n",
    "1. 1000000 chunk -> 22min 22sec\n",
    "'''\n",
    "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "raw_data_directory = BOOKS_RAW_DIR\n",
    "clean_data_directory = BOOKS_CLEAN_DIR\n",
    "sentiment_data_directory = BOOKS_SENTIMENT_DIR\n",
    "\n",
    "chunk_size = 1000000\n",
    "num_cores = int(mp.cpu_count()/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f951cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(url, chunk_size, num_cores, raw_data_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe328f04",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f706d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(dictionary: dict):\n",
    "    return dictionary.get(\"Format:\") if isinstance(dictionary, dict) else None\n",
    "\n",
    "def filter_comment_length(reviews: pd.DataFrame, minimum: int) -> pd.DataFrame:\n",
    "    reviews = reviews.copy()\n",
    "    reviews[\"review_len\"] = reviews[\"review_text\"].str.split().str.len()\n",
    "    reviews = reviews.loc[(reviews[\"review_len\"] > minimum)]\n",
    "    return reviews\n",
    "\n",
    "def remove_irrelevant_info(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"http\\S+|www.\\S+|#\\S+|<.*?>|\\(|\\)|\\d+\", \"\", x)) # Cleans up URL, hashtags, parenthesis, and numbers.\n",
    "\n",
    "def reduce_characters(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"[^\\w\\s]|(.)\\1+\", \"\", x)) # Remove excessive punctuation and repeated characters\n",
    "\n",
    "def normalize_encoding(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")) # Encodes and Decodes the data so that we have consistency in text\n",
    "        \n",
    "def clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"overall\": \"rating\",\n",
    "        \"reviewTime\": \"review_date\",\n",
    "        \"reviewerID\": \"reviewer_id\",\n",
    "        \"asin\": \"product_id\",\n",
    "        \"reviewText\": \"review_text\",\n",
    "    })\n",
    "\n",
    "    df[\"vote\"] = df[\"vote\"].fillna(0)\n",
    "    df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(\"str\").str.replace(\",\",\"\")).astype(\"int32\")\n",
    "    df = df[df[\"vote\"] >= 5]\n",
    "    df = filter_comment_length(df, 20)\n",
    "    df = df.drop([\"unixReviewTime\", \"image\", \"summary\", \"reviewerName\"], axis=1)\n",
    "    \n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], format=\"%m %d, %Y\")\n",
    "    df[\"style\"] = df[\"style\"].apply(extract_value)\n",
    "\n",
    "    df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
    "    df[\"review_text\"] = remove_irrelevant_info(df[\"review_text\"])\n",
    "    df[\"review_text\"] = reduce_characters(df[\"review_text\"])\n",
    "    df[\"review_text\"] = normalize_encoding(df[\"review_text\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdcaa284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_0.json DIMENSIONS: (103953, 9)\n",
      "chunk_1.json DIMENSIONS: (87089, 9)\n",
      "chunk_2.json DIMENSIONS: (111328, 9)\n",
      "chunk_3.json DIMENSIONS: (98099, 9)\n",
      "chunk_4.json DIMENSIONS: (75862, 9)\n",
      "chunk_5.json DIMENSIONS: (100224, 9)\n",
      "chunk_6.json DIMENSIONS: (80360, 9)\n",
      "chunk_7.json DIMENSIONS: (111614, 9)\n",
      "chunk_8.json DIMENSIONS: (115463, 9)\n",
      "chunk_9.json DIMENSIONS: (98534, 9)\n",
      "chunk_10.json DIMENSIONS: (147581, 9)\n",
      "chunk_11.json DIMENSIONS: (92743, 9)\n",
      "chunk_12.json DIMENSIONS: (74607, 9)\n",
      "chunk_13.json DIMENSIONS: (87355, 9)\n",
      "chunk_14.json DIMENSIONS: (48748, 9)\n",
      "chunk_15.json DIMENSIONS: (51401, 9)\n",
      "chunk_16.json DIMENSIONS: (42985, 9)\n",
      "chunk_17.json DIMENSIONS: (44389, 9)\n",
      "chunk_18.json DIMENSIONS: (124999, 9)\n",
      "chunk_19.json DIMENSIONS: (89712, 9)\n",
      "chunk_20.json DIMENSIONS: (91282, 9)\n",
      "chunk_21.json DIMENSIONS: (38537, 9)\n",
      "chunk_22.json DIMENSIONS: (46373, 9)\n",
      "chunk_23.json DIMENSIONS: (55919, 9)\n",
      "chunk_24.json DIMENSIONS: (65645, 9)\n",
      "chunk_25.json DIMENSIONS: (29545, 9)\n",
      "chunk_26.json DIMENSIONS: (43437, 9)\n",
      "chunk_27.json DIMENSIONS: (2048, 9)\n"
     ]
    }
   ],
   "source": [
    "raw_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(raw_data_directory)))]\n",
    "\n",
    "for raw_file_name in raw_file_names:\n",
    "    raw_df_list = []\n",
    "    raw_df = pd.read_json(os.path.join(raw_data_directory, raw_file_name), lines=True)\n",
    "   \n",
    "    raw_df = clean_chunk(raw_df)\n",
    "    print(f\"{raw_file_name} DIMENSIONS: {raw_df.shape}\")\n",
    "    clean_dir = os.path.join(clean_data_directory, raw_file_name)\n",
    "    raw_df.to_json(clean_dir, orient=\"records\")\n",
    "\n",
    "    del raw_df\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eae861a-afe7-4a17-97a4-a1dea5fc2ee9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b721b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_text(column: pd.Series, batch_size: int) -> list:\n",
    "    batches = np.array_split(np.array(column.values), int(np.ceil(len(column)/ batch_size)))\n",
    "    batches = [batch.tolist() for batch in batches]\n",
    "    return batches\n",
    "\n",
    "def load_sentiment_model(model: str):\n",
    "    return TextClassifier.load(model)\n",
    "\n",
    "def is_float(value):\n",
    "      try:\n",
    "        float(value)\n",
    "        return True\n",
    "      except ValueError:\n",
    "        return False\n",
    "      \n",
    "def analyze_sentiment(model, batch: np.ndarray):\n",
    "    score = []\n",
    "    for sentence in batch:\n",
    "        sentence = Sentence(sentence)\n",
    "        model.predict(sentence, verbose=True)\n",
    "        process = re.sub(r\"\\(|\\)\",\"\",str(sentence.labels[0]))\n",
    "        number = [float(s) for s in  process.split() if is_float(s) is True]\n",
    "        if \"POSITIVE\" in process:\n",
    "            score.append(number[0])\n",
    "        elif \"NEGATIVE\" in process:\n",
    "            score.append(-number[0])\n",
    "    return score\n",
    "   \n",
    "def process_batches(models: list, batches: list):\n",
    "    models_loaded = [load_sentiment_model(model) for model in models]\n",
    "    sentiment_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers= 4) as executor:\n",
    "        sentiment_tasks = [executor.submit(analyze_sentiment, model, batch) for model, batch in zip(models_loaded, batches)]\n",
    "        concurrent.futures.wait(sentiment_tasks)\n",
    "        [sentiment_results.extend(task.result()) for task in sentiment_tasks]\n",
    "        return sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09444c36-567e-41fc-9bf6-4e82688ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(raw_data_directory)))]\n",
    "\n",
    "for clean_file_name in clean_file_names[0:1]:\n",
    "    clean_df = pd.read_json(os.path.join(clean_data_directory, clean_file_name), orient=\"columns\")\n",
    "    batch_size = math.ceil(len(clean_df)/num_cores) \n",
    "    batches = batch_text(clean_df[\"review_text\"], batch_size)\n",
    "    clean_df[\"sentiment_score\"] = process_batches([\"en-sentiment\"] * len(batches), batches)\n",
    "\n",
    "    sentiment_dir = os.path.join(sentiment_data_directory, clean_file_name)\n",
    "    clean_df.to_json(sentiment_dir, orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
