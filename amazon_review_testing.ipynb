{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175e7786-8e51-4e84-849b-9d00e5d4c92a",
   "metadata": {},
   "source": [
    "# Recommender System with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "951e16ec",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "983b6c02",
   "metadata": {},
   "source": [
    "### Packages & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc723e1-d85f-40b3-91fa-0e8f85395711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import psutil\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "import urllib.request\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "\n",
    "import gzip\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9021e6d4",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83509cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.join(\"D:\" + os.sep, \"Code\", \"PYTHON\", \"Amazon_Recommender_System\")\n",
    "\n",
    "CODE_DIR = os.path.join(MAIN_DIR, \"Code\")\n",
    "\n",
    "ANALYSIS_DIR = os.path.join(MAIN_DIR, \"Analysis\")\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"Data\")\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"Raw\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_DIR, \"Clean\")\n",
    "\n",
    "BOOKS_RAW_DIR = os.path.join(RAW_DATA_DIR, \"Books\")\n",
    "BOOKS_CLEAN_DIR = os.path.join(CLEAN_DATA_DIR, \"Books\")\n",
    "BOOKS_SENTIMENT_DIR = os.path.join(BOOKS_CLEAN_DIR, \"Sentiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71a50b4e",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a6c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_list: list) -> None:\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def is_directory_empty(dir_path: str) -> bool:\n",
    "    return len(os.listdir(dir_path)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3231a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory([CODE_DIR, \n",
    "                  ANALYSIS_DIR,\n",
    "                  DATA_DIR, \n",
    "                  RAW_DATA_DIR, \n",
    "                  CLEAN_DATA_DIR,\n",
    "                  BOOKS_RAW_DIR,\n",
    "                  BOOKS_CLEAN_DIR,\n",
    "                  BOOKS_SENTIMENT_DIR,\n",
    "                  ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53f8033",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca015dd",
   "metadata": {},
   "source": [
    "### Loading Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc72a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More memory efficient version. Takes longer but is far more consistent than the previous version. \n",
    "def process_data(url: str, chunk_size: int, num_workers: int, output_dir: str) -> None:\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with gzip.open(response, \"rt\") as gz_file:\n",
    "            chunk = []\n",
    "            chunk_count = 0\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                for line in gz_file:\n",
    "                    chunk.append(line)\n",
    "\n",
    "                    if len(chunk) == chunk_size:\n",
    "                        executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "                        chunk = []\n",
    "                        chunk_count += 1\n",
    "\n",
    "                # Process the remaining lines in the last chunk\n",
    "                if chunk:\n",
    "                    executor.submit(process_chunk, list(chunk), os.path.join(output_dir, f\"chunk_{chunk_count}.json\"))\n",
    "\n",
    "def process_chunk(chunk: list, filename: str) -> None:\n",
    "    with open(filename, 'w') as file:\n",
    "        file.writelines(chunk)\n",
    "    print(f\"Processed chunk: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "----- PROCESS_DATA -----\n",
    "GAME URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "BOOK URL \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "EXPERIMENTS FOR 27 MILLION\n",
    "----- THREAD CSV_FILE -----\n",
    "1. 100000 chunk -> 22min 26sec\n",
    "\n",
    "----- THREAD JSON_FILE RUNTIMES-----\n",
    "1. 100000 chunk -> 20min 55sec\n",
    "2. 100000 chunk -> 49min 1sec -> No idea why this happened on a fresh start\n",
    "3. 500000 chunk -> 18min 45sec\n",
    "4. 1000000 chunk -> 17min 5sec\n",
    "\n",
    "NEW STABLE IMPLEMENTATION\n",
    "1. 1000000 chunk -> 22min 22sec\n",
    "'''\n",
    "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
    "\n",
    "raw_data_directory = BOOKS_RAW_DIR\n",
    "clean_data_directory = BOOKS_CLEAN_DIR\n",
    "sentiment_data_directory = BOOKS_SENTIMENT_DIR\n",
    "\n",
    "chunk_size = 1000000\n",
    "\n",
    "NUM_CORES = math.ceil(mp.cpu_count()/2)\n",
    "\n",
    "CPU_ARGS = {\n",
    "    \"cpu_target\" : 0.80, # Set some initial CPU load values as a CPU usage goal\n",
    "    \"thread_group_size\" : NUM_CORES, # When CPU load is significantly low, start this number of threads\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f951cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(url, chunk_size, NUM_CORES, raw_data_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe328f04",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f706d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(dictionary: dict):\n",
    "    return dictionary.get(\"Format:\") if isinstance(dictionary, dict) else None\n",
    "\n",
    "def filter_comment_length(reviews: pd.DataFrame, minimum: int) -> pd.DataFrame:\n",
    "    reviews = reviews.copy()\n",
    "    reviews[\"review_len\"] = reviews[\"review_text\"].str.split().str.len()\n",
    "    reviews = reviews.loc[(reviews[\"review_len\"] > minimum)]\n",
    "    return reviews\n",
    "\n",
    "def remove_irrelevant_info(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"http\\S+|www.\\S+|#\\S+|<.*?>|\\(|\\)|\\d+\", \"\", x)) # Cleans up URL, hashtags, parenthesis, and numbers.\n",
    "\n",
    "def reduce_characters(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: re.sub(r\"[^\\w\\s]|(.)\\1+\", \"\", x)) # Remove excessive punctuation and repeated characters\n",
    "\n",
    "def normalize_encoding(reviews: pd.Series) -> pd.Series:\n",
    "    return reviews.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")) # Encodes and Decodes the data so that we have consistency in text\n",
    "        \n",
    "def clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"overall\": \"rating\",\n",
    "        \"reviewTime\": \"review_date\",\n",
    "        \"reviewerID\": \"reviewer_id\",\n",
    "        \"asin\": \"product_id\",\n",
    "        \"reviewText\": \"review_text\",\n",
    "    })\n",
    "\n",
    "    df[\"vote\"] = df[\"vote\"].fillna(0)\n",
    "    df[\"vote\"] = pd.to_numeric(df[\"vote\"].astype(\"str\").str.replace(\",\",\"\")).astype(\"int32\")\n",
    "    df = df[df[\"vote\"] >= 5]\n",
    "    df = filter_comment_length(df, 20)\n",
    "    df.drop([\"unixReviewTime\", \"image\", \"summary\", \"reviewerName\"], axis=1, inplace=True)\n",
    "    \n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], format=\"%m %d, %Y\")\n",
    "    df[\"style\"] = df[\"style\"].apply(extract_value)\n",
    "\n",
    "    df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
    "    df[\"review_text\"] = remove_irrelevant_info(df[\"review_text\"])\n",
    "    df[\"review_text\"] = reduce_characters(df[\"review_text\"])\n",
    "    df[\"review_text\"] = normalize_encoding(df[\"review_text\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcaa284",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m raw_file_name \u001b[39min\u001b[39;00m raw_file_names:\n\u001b[0;32m      4\u001b[0m     raw_df_list \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m     raw_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(raw_data_directory, raw_file_name), lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      7\u001b[0m     raw_df \u001b[39m=\u001b[39m clean_chunk(raw_df)\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mraw_file_name\u001b[39m}\u001b[39;00m\u001b[39m DIMENSIONS: \u001b[39m\u001b[39m{\u001b[39;00mraw_df\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\json\\_json.py:760\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    758\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 760\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[0;32m    761\u001b[0m     path_or_buf,\n\u001b[0;32m    762\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m    763\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[0;32m    764\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    765\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[0;32m    766\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[0;32m    767\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[0;32m    768\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[0;32m    769\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m    770\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    771\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[0;32m    772\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    773\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    774\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    775\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    776\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[0;32m    777\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    778\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    779\u001b[0m )\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[0;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\json\\_json.py:862\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    861\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m--> 862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_data(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\json\\_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunksize \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows):\n\u001b[0;32m    873\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 874\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunksize \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows):\n\u001b[0;32m    876\u001b[0m     data \u001b[39m=\u001b[39m StringIO(data)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(raw_data_directory)))]\n",
    "\n",
    "for raw_file_name in raw_file_names:\n",
    "    raw_df_list = []\n",
    "    raw_df = pd.read_json(os.path.join(raw_data_directory, raw_file_name), lines=True)\n",
    "   \n",
    "    raw_df = clean_chunk(raw_df)\n",
    "    print(f\"{raw_file_name} DIMENSIONS: {raw_df.shape}\")\n",
    "    clean_dir = os.path.join(clean_data_directory, raw_file_name)\n",
    "    raw_df.to_json(clean_dir, orient=\"records\")\n",
    "\n",
    "    del raw_df\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eae861a-afe7-4a17-97a4-a1dea5fc2ee9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b721b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_text(column: pd.Series, batch_size: int) -> list:\n",
    "    batches = np.array_split(np.array(column.values), int(np.ceil(len(column)/ batch_size)))\n",
    "    batches = [batch.tolist() for batch in batches]\n",
    "    return batches\n",
    "\n",
    "def load_sentiment_model(model: str):\n",
    "    return TextClassifier.load(model)\n",
    "\n",
    "def is_float(value):\n",
    "      try:\n",
    "        float(value)\n",
    "        return True\n",
    "      except ValueError:\n",
    "        return False\n",
    "      \n",
    "def analyze_sentiment(model, batch: np.ndarray):\n",
    "    score = []\n",
    "    for sentence in batch:\n",
    "        sentence = Sentence(sentence)\n",
    "        model.predict(sentence, verbose=True)\n",
    "        process = re.sub(r\"\\(|\\)\",\"\",str(sentence.labels[0]))\n",
    "        number = [float(s) for s in  process.split() if is_float(s) is True]\n",
    "        if \"POSITIVE\" in process:\n",
    "            score.append(number[0])\n",
    "        elif \"NEGATIVE\" in process:\n",
    "            score.append(-number[0])\n",
    "    return score\n",
    "   \n",
    "def process_batches(models: list, batches: list):\n",
    "    models_loaded = [load_sentiment_model(model) for model in models]\n",
    "    \n",
    "    sentiment_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers= 4) as executor:\n",
    "        sentiment_tasks = [executor.submit(analyze_sentiment, model, batch) for model, batch in zip(models_loaded, batches)]\n",
    "        concurrent.futures.wait(sentiment_tasks)\n",
    "        [sentiment_results.extend(task.result()) for task in sentiment_tasks]\n",
    "        return sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09444c36-567e-41fc-9bf6-4e82688ae8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great day for up is a unique dr seu bk and you can te this just by lking at the coverthat is because while the bk is wrien by dr seu it features the joy drawings of the english artist quentin blakeuntil this point every time i have read a bk wrien by dr seu it was also iustrated by dr seu and when somebody else did the drawings dr seu used the name theo lesieg which is geisel backwardsso the fact that this is a real dr seu bk drawn by somebody else is prey special\\nthis bright and early bk provides rhymed text and iustrations introducing the many meanings of the word up as seu and blake show begiing readers that this is a great day for upyou get the point half way through the bk but lile kids should be able to hand on longer especiay when they are reading the bk for themselvesbesides by the end of great day for up we get to the point where everyone on earth is up with one very important and rather ironic exception\\nas with a of the bright and early bks for begiing begiers what you have here is a brief and fuy story where the words are few and easy there is a catchy rhythm and the pictures are hay and colorful clues to the textthese are designed for an even lower age group than the bright and early bks that foowed the cat in the hat which was the hay poer of its day when it came to encouraging even preschlers to discover the delights of reading for themselvesthis is not one of the most interesting volumes in the series but overa these bks were a delight', 'children are normay confused by the multiplicity of meanings that a simple word can have when they start readingdr seu has wrien a bk here to can aow you to help your child understand that problem by lking at what up can mean in dierent contextsthe beautiful watercolor and inked outline iustrations by quentin blake provide great context for these meanings\\nup upthe sun is geing up\\nthe sun gets up\\nso up with you\\nthus this bk beginsyou can s that dr seu has already coected the idea of the sun rising above the horizon in the east with your rising from your bedthe bk goes on to explore a the things that can risethese includes ears on a rait hands whiskers and eyes\\nonce he goes into eyes he then points out that many living creatures have eyes including worms frogs buerflies whales and insects\\nthen dr seu returns to up and gives new meaningsthese include taking something from a lower position to a higher one like puing your ft up by walking on your hands throwing things into the air like bas guiding things into the air like kites climbing like going up a mountain  mt dimadilts in this case and building mechanisms that can go up like an elevator or a feis whl\\nthen he returns ingeniously to the original concept of arising from bed\\nwake ever person\\npig and pup\\nti everyone\\non earth is up\\nthen comes the surprise ending that wi kp you and your child chuckling for years\\nat first you may just think the ending is there simply for humor but it actuay extends your childs understanding of what saying up means in terms of cause and eect\\nthe bk has a of the qualities i lk for in an early readerthe language is simplethere is a limited vocabulary of short wordsthe iustrations tie in clearly to the wordsthe story is interesting humorous and upbeata child can learn to recognize the key word up in just a few readings\\nafter your child has mastered this wonderful story i suest that you encourage your child to use this bk to identify synonyms for up which wi extend the value of the bkfor example you can use arise or rise in many of the contextsthen you can discu how a speaker or a writer chses which word version of a concept to use\\nmay a of your childs learning experiences be up to the ones available in this bk', 'the carpet wars is a sampler of informal writing from australian journalist and avid carpet coector christopher kreer over ten years in central asiasince most of it was wrien and concerns events before  when the area was not established in the wests cultural radar as it is today it gives a view of the region that is uncluered by hindsight rvaluations\\nkreer writes of his time in afghanistan pakistan tajikstan kashmir and iran giving us colorful and nonjournalistic slices of life from each regionhe enlivens his writings with vivid character studies of those he met on his travels from dignitaries like ifated afghan dictator mohaed najibuah and legendary gueria ahmad shah maoud to various carpet dealers kreer got to know over his time in the regionbetwn these character sketches and kreers anecdotes he delivers measured doses of regional history and politics and he imparts a surprising amount of information about his favorite hoy the asian carpet\\nthe result is more than just some very entertaining travel writingkreers lively and discursive work also functions as an exceent introduction to the central asian economy and politicsbesides being for those who just like to read about travel in interesting foreign parts the carpet wars wi also be useful for nonscholars who want to have some idea how movements like the taliban came to be but want to take a spnful of sugar with this medicine\\nkreers bk also taught me that i never know enough to bargain eectively for an asian carpet  but his rueful and wry work also admits that there is a certain pleasure in being cheated', 'i finished this bk about six wks ago and i cant stop thinking about it\\nso often travel literaturetype bks by westerners in these kinds of faro places can be either t clever cynical or condescending at one end of the scale or at the other end t reverent with a reverence that sms to reay be an ihatewhereiamfrom complex both extremes can get tiring prey quickly\\nthe carpet wars was exactly in the mile and it was fascinating it was extremely informative about the history politics religion and yes even the carpets of the region from pakistan to irancarpets were merely the thread so to speak that held the several firsthand aounts of travels to the region\\nkreer is a master story teer and very fuysometimes it was hard to te what was more enjoyable the story he was teing or the way he was teing it\\nhis aounts of places with which he is very familiar are told in the rich tones of a dp aectionwhen he is in a new place like isfahan the aount is in the vivid colors of someone sing something for the first time creating some of the best travel eays i have ever read seven wks ago isfahan was just an exotic name to me now its at the top of places i hope i can s before i die\\nits hard to say what recoends this bk more the fact that it is throughly enjoyable or dply infomrative\\ni havent read mr kreers bk about laos but it is probably prey gdbks like the carpet wars dont stick with you so long by aident', 'anyone interested in the fine art of rug weaving the cultures in which oriental carpets originate the geography of ancient trade routes including the silk road the history economics and politics of the mile east and present day travel through the strifetorn region wi find iense treasure in christopher kreers the carpet warsthe early muslims inhabited lands where people were born on carpets prayed on them and covered their tombs with themfor centuries carpets have bn a cuency and an export among the first coodities of a globalized trading system writes the author who has spent ten years in asia reporting for the australian prehe uses oriental rugs as his motif for writing about his travels in afghanistan pakistan iraq iran tajikistan kashmir and the former soviet sateite countries of central asiahere this reviewer admits his interest enfolds some bias because of my own travels through the region as a peace corps worker in le turbulent timesalso i have the gd fortune of working in a store where a wide variety of the very finest examples of oriental carpets are sold\\n in this bk we read that second only to oil hand made carpets are the regions principal export and were so long before marco polo made his famous travels along the silk roadcarpets created by various quaeling factions from the mile and near east are the focus for reteing how the fighting clans have damaged the carpet trade eectively wiping out the mile and uer cla of society and left aaing poverty and misery in its wakekreer describes how that even in the midst of war and turmoil a bazr wi spring up during breaks in the fighting and the carpet merchants wi quickly resume busine as if nothing had haeneda disaointment for me was that the author omits a description of the many varieties and techniques of rug making he remains focused on his travels through the islamic world giving us the benefit of his first hand witne to the misery\\n believing that only aah can create anything perfect the muslim carpet makers often wi deliberately craft a minor flaw in their handiwork that only a practiced eye might discernalso we learn that many rugs woven by people living under the dure of conflict wi reflect their anxieties and turmoil through the symbols of war  airplanes helicopters tanks and gunsbut the rugs also wi contain symbols of their makers traumatic lives not altogether discernible or understdlike the great paintings of the renaiance these works of art may never be fuy comprehendedit is enough that fortunate owners of hand knoed and woven rugs might areciate not only their beauty but also how they portray the soulful dper meaning of the lives of their creators leaving a legacy for generations to comethis bk is an armchair journey of iense interesthighly recoended\\nmore about this reviewer on the', 'christopher kreers bk takes you on a journey through the central asian countries most frequently in the news today and provides an incomparable insight the largest and first section is an aount of events in afghanistan which he has witneed firsthand as a foreign coespondent\\nthis bk is no dry history nor is it merely a travelogue nor is it merely an extended piece of journalism\\nkreer comes to know and befriend people of dierent backgrounds within the region and it is their stories as we as the carpet trade and stories of emblematic carpets through which the naative is woven we care about the future of the peoples of the region because we care about what becomes of kreers friends\\nwhat christopher has managed to do is to make the internecine politics the inhumanities the brutalities comprehensible through his humanisation of peoples who might in leer hands be reduced to the merely exotic or even worse unknowable and inhuman\\nearlier this year i read unexpected light travels in afghanistan by jason eiot\\ni thoroughly recoend both these bks if you desire to reach some understanding of a region of such importance to us a', 'i love this series but this entry is not hiermans best stuim not sure if hes ruing out of steam or if the life stories of joe leaphorn and jim ch dont aow much development at this pointthe dense subploing and evocative details about navajo life so evident in the best leaphornch stories are mostly miing herehierman sms to have bn motivated mainly by a desire to wrap up the jim chbernie manuelito relationship which he does but not in a particularly original wayif you are new to this series try the bks wrien in the s to get a beer sense of hiermans considerable talent the dark wind is probably the best of the best', 'i can almost sme the dry red clay and s that beautiful blue sky as i read this bk this bk is a quick read but has some interesting twists to iti can close my eyes and remember traveling a sma state highway along the great divide on my way to s the vla near datil nmi had read only a couple of the jim chleaphorn bks when i visited gaup new mexico inandbut i almost expected to s one of the bks characters aear at the giant truck stop co shop or one of the tiny diners we saw on hwy jim sms to be taking a big step in this bk so i wi wait patiently for another of these charming suspensful novels and i hope my husband gets me this one on tape tthank you tony hierman', 'the sinister pig a term which has a couple of meanings in the bk is another fine aition to tony hiermans joe leaphornjim ch seriesthis one involves a somewhat more complex plot than most of the previous ones and as for character development mr hierman sms to take it for granted that the readers of this bk have also read at least some of the others and that therefore he does not nd to go into t much detail as to the personalities of his charactershaving said that though i enjoyed the bk very much and my only real complaint is as with the wailing wind it is at aboutpages just t shortbut it is always a rare treat to be reunited with our old friends from the navajo tribal police and as an aed bonus this edition features a hay ending romanticay for the everlovelorn jim chheres hoping mr hierman kps the leaphornch series coming and that he as a hundred or so pages to his next tale', 'the sinister pig is another in hiermans longruing series of mystery novels centering upon the now retired but hardly inactive lieutenant joe leaphorn and sergeant jim ch of the navajo tribal policethis time the plot is in part inspired by the continuing scandal over the mismanagement  embelement and outright theft may be closer to the point  of funds due to southwest indian tribes for oil gas and coal taken from their reservations under federal auspicesan investigator sent by a powerful washington dc senator to nose around turns up dead with a buet in his backit is jim chs case  or at least as much of the case as the fbi wi let him handle  but it is iediately clear that somebody with high coections back in washington wants the investigation squelched\\nmeanwhile jim ch has something else on his mindbernadee manuelito formerly an oicer in the navajo tribal police has taken a new job with the border patrolmiles away just when ch was working up his resolve to make his personal interest clear to herand now bernie has stumbled on some mysterious goingson along the mexican border that might tie in to the unsolved murder back home\\nhierman departs somewhat from his usual format by writing several chapters from outside the viewpoint of leaphorn and ch and bernie manuelitounlike in most hierman novels we very quickly learn who the bad guys are although a mystery remains until the final chapters as to exactly what they are doingin general hiermans viains are not especiay viainous their motivations often arising from quite ordinary circumstances that lead them into crimes they never intendedbut in the sinister pig the chief viain is as close to plain evil as hierman is ever likely to get\\none disaointment an element which usuay sets hiermans mystery novels apart from a others is their exploration of the culture and religion of the navajos and their indian neighbors this being integral to the bk plot and often crucial to the solution of the mystery at handin the present novel we s almost nothing of this except for some peripheral maers that only touch upon jim ch himselfwashington powerbrokers are a le engaging group than the people of the big rez\\nthe sinister pig is not the best of hierman to be sure and it might be argued that it works primarily as simply being part of a continuing series about characters to whom we have come to fl close over the yearsbut a hierman bk that is not amongst his best work is sti a gd mysteryand readers who count joe leaphorn and jim ch as literary friends wi want to find what has now haened in their personal lives\\nhiermans navajo novels have continuing background stories that develop from novel to novel over timetherefore readers new to hierman would be we advised to begin not with this latest novel but back at the begiing of the series geing to know the characters as their lives evolvetheres plenty of gd reading to be had along the way', 'i have read every leaphornch bk by hierman more than once and am a great fan i just finished this bk and discued it with a friend we both agrd it was so disaointing we caot believe it was wrien by the same person the writing was awkward and heavyhanded by the second page i suspected something was wrong here it reads like some cheap harlequin mysteryromance\\n\\nthe sensitive handling of navajo spiritual beliefs the beautiful rendering of the landscape the terse but meaningful conversations are a miing\\n\\na i can say is the publishers nd to pay more aention to future manuscripts and insure that a writer of tony hiermans caliber is not aowed to decline in the public eye he has given us much enjoyment he deserves beer\\n\\nand if someone else is fleshing out hiermans plot lines the public should be warned', 'if tony hierman wrote this novel novea actuay i sure cant te i have bn a chleaphorn fan for many years but this novel is not in the same league with hiermans other work it is reminiscent of a b movie instead of building suspense the plot is laid out a t plainly the characters are unreal and the chbernie romance doesnt ring truei am amazed that it was published']\n"
     ]
    }
   ],
   "source": [
    "clean_file_names = [f\"chunk_{file_num}.json\" for file_num in range(len(os.listdir(raw_data_directory)))]\n",
    "\n",
    "for clean_file_name in clean_file_names[0:1]:\n",
    "    clean_df = pd.read_json(os.path.join(clean_data_directory, clean_file_name), orient=\"columns\")[0:24]\n",
    "    batch_size = math.ceil(len(clean_df)/2) \n",
    "    \n",
    "    batches = batch_text(clean_df[\"review_text\"], batch_size)\n",
    "    # models_loaded = [load_sentiment_model(model) for model in [\"en-sentiment\"] * len(batches)]\n",
    "    # clean_df[\"sentiment_score\"] = process_batches([\"en-sentiment\"] * len(batches), batches)\n",
    "    model = TextClassifier.load(\"en-sentiment\")\n",
    "    my_queue = \n",
    "    for batch in batches:\n",
    "        batch\n",
    "    start_thread_process(model, batches)\n",
    "    # sentiment_dir = os.path.join(sentiment_data_directory, clean_file_name)\n",
    "    # clean_df.to_json(sentiment_dir, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(model, batch: np.ndarray):\n",
    "    score = []\n",
    "    for sentence in batch:\n",
    "        sentence = Sentence(sentence)\n",
    "        model.predict(sentence, verbose=True)\n",
    "        process = re.sub(r\"\\(|\\)\",\"\",str(sentence.labels[0]))\n",
    "        number = [float(s) for s in  process.split() if is_float(s) is True]\n",
    "        if \"POSITIVE\" in process:\n",
    "            score.append(number[0])\n",
    "        elif \"NEGATIVE\" in process:\n",
    "            score.append(-number[0])\n",
    "    return score\n",
    "\n",
    "# Spool down the thread balance when load is too high\n",
    "def spool_down_load_balance(args_array):\n",
    "    one_minute_load_average = psutil.getloadavg()[0] / NUM_CORES # Calculate the short term load average of past minute\n",
    "    # If load balance above the max return True to kill the process\n",
    "    if one_minute_load_average > args_array['cpu_target']:\n",
    "        print(\"-Unacceptable load balance detected. Killing process \" + str(os.getpid()) + \"...\")\n",
    "        return True\n",
    "\n",
    "# Load balancer function\n",
    "def load_balance(item_queue, args_array):\n",
    "    print(\"[Starting load balancer...]\")\n",
    "    # While there are still items in the queue\n",
    "    while not item_queue.empty():\n",
    "        print(\"[Calculating load balance...]\")\n",
    "        one_minute_load_average = psutil.getloadavg()[0] / NUM_CORES # Check the 1 minute average CPU load balance and returns 1,5,15 minute load averages\n",
    "        # If the load average is much less than the target, start a group of new threads\n",
    "        if one_minute_load_average < args_array['cpu_target'] / 2:\n",
    "            # Print message and log that load balancer is starting another thread group\n",
    "            print(\"Starting another thread group due to low CPU load balance of: \" + str(one_minute_load_average * 100) + \"%\")\n",
    "            time.sleep(5)\n",
    "        \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=args_array['thread_group_size']) as executor:\n",
    "                for _ in range(args_array['thread_group_size']):\n",
    "                    executor.submit(analyze_sentiment, model, batch) \n",
    "            # Allow the added threads to have an impact on the CPU balance\n",
    "            # before checking the one minute average again\n",
    "            time.sleep(20)\n",
    "\n",
    "        # If the load average is less than the target, start a single thread\n",
    "        elif one_minute_load_average < args_array['cpu_target']:\n",
    "            # Print message and log that load balancer is starting another thread\n",
    "            print(\"Starting another single thread due to low CPU load balance of: \" + str(one_minute_load_average * 100) + \"%\")\n",
    "            # Start another thread\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                executor.submit(main_process, item_queue.get())\n",
    "            # Allow the added thread to have an impact on the CPU balance\n",
    "            # before checking the one minute average again\n",
    "            time.sleep(20)\n",
    "\n",
    "        else:\n",
    "            # Print CPU load balance\n",
    "            print(\"Reporting stable CPU load balance: \" + str(one_minute_load_average * 100) + \"%\")\n",
    "            # Sleep for another minute\n",
    "            time.sleep(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
